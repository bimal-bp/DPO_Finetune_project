{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imporing lib..................\n",
    "import os \n",
    "from dataclasses import dataclass ,field \n",
    "from typing import dict , Optional \n",
    "import torch \n",
    "from datasets import Dataset , load_dataset \n",
    "from peft import AutoPeftModelForCausalLM,LoraConfig \n",
    "from transformers import AutoTokenizer ,HfArgumentParser ,TrainingArguments\n",
    "\n",
    "from trl import DPOTrainer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATSEt preocessing \n",
    "def dpo_data(train_or_val):\n",
    "    dataset = load_dataset(\n",
    "        \"Dahoas/full-hh-rlhf\",\n",
    "        split=\"train\",\n",
    "        use_auth_token=True\n",
    "    )\n",
    "    original_columns = dataset.column_names \n",
    "    \n",
    "\n",
    "    def return_prompt_and_responses(samples):\n",
    "        return{\n",
    "            \"prompt\":[prompt for prompt in samples[\"prompt\"]],\n",
    "            \"choosen\":samples[\"choosen\"]\n",
    "            \"rejected\":samples[\"rejected\"]\n",
    "        }\n",
    "    return dartaset.map(\n",
    "        return_prompt_and_responses,\n",
    "        batched=True,\n",
    "        remove_columsn=original_columns\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 1.load finetuned save model \n",
    "    model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "        \"content/sft_santacoder1b\",\n",
    "        low_cpu_mem_usage=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        load_in_4bit=True\n",
    "    )\n",
    "    model.config.use_cache=False \n",
    "\n",
    "    model_ref = AutoPeftModelForCausalLM.from_pretrained(\n",
    "        \"/content/sft_santacoder1b/\",\n",
    "        low_cpu_mem_usage=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        load_in_4bit=True\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"/content/sft_santocoder1b/\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    train_dataset = dpo_data(\"train\")\n",
    "    train_dataset = train_dataset.filter(\n",
    "        lambda x:len(x[\"prompt\"] + len(x[\"chosen\"])<=256\n",
    "        and len(x[\"prompt\"])+len(x[\"rejected\"]<=256))\n",
    "    )\n",
    "\n",
    "    eval_dataset = dpo_data(\"val\")\n",
    "    eval_dataset = eval_dataset.filter(\n",
    "        lambda x: len(x[\"prompt\"]) +len(x[\"chosen\"]) <= 256\n",
    "        and len(x[\"prompt\"]) + len(x[\"rejected\"]) <= 256 \n",
    "    )\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        max_steps=505,\n",
    "        logging_steps=10,\n",
    "        save_steps = 500,\n",
    "        gradient_accumulation_steps=4,\n",
    "        gradient_checkpointing=True,\n",
    "        learning_rate = 2e-4,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=100,\n",
    "        output_dir=\"dpo_santscoder1b\",\n",
    "        report_to=\"tensorboard\",\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        warmup_steps =2,\n",
    "        optim=\"paged_adamw_32bit\",\n",
    "        fp16=True,\n",
    "        remove_unused_columns=False,\n",
    "        run_name = \"dpo_llama2\"\n",
    "    )\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules=[\"c_attn\",\"c_proj\"],\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    # last dpo trainer \n",
    "    dpo_trainer = DPOTrainer(\n",
    "        model,\n",
    "        model_ref,\n",
    "        args=training_args,\n",
    "        beta=0.1,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        peft_config=peft_config,\n",
    "        max_prompt_length = 128,\n",
    "        max_length = 256,\n",
    "    )\n",
    "\n",
    "    # training\n",
    "    dpo_trainer.train()\n",
    "    dpo_trainer.save_model(\"dpo_sansatcoder1b\")\n",
    "\n",
    "    # save\n",
    "    output_dir=os.path.join(\"dpo_santacoder1b\",\"final_checkpoint\")\n",
    "    dpo_trainer.model.save_pretrained(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"/content/sft_santacoder1b/final_merged_checkpoint/\", return_dict=True, torch_dtype=torch.float16\n",
    ")\n",
    "model = PeftModel.from_pretrained(model, \"/content/dpo_santacoder1b/final_checkpoint/\")\n",
    "model.eval()\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "model.save_pretrained(\"/content/dpo_santacoder1b/final_merged_checkpoint\")\n",
    "tokenizer.save_pretrained(\"/content/dpo_santacoder1b/final_merged_checkpoint\")\n",
    "model.push_to_hub(\"dpo-santacoder1b\")\n",
    "tokenizer.push_to_hub(\"dpo-santacoder1b\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
